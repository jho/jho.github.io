<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }

      table {
        border-collapse: collapse;
      }
      th {
        padding: 5px;
        border: 1px solid black;
      }
      td {
        padding: 5px;
        border: 1px solid grey;
      }

      .big {
        font-size: xx-large;
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Title

---

# Agenda

1. Introduction
2. Deep-dive
3. ...

---

# Introduction

---

# Problem 1: 
* Data distributed evenly across all these expensive servers
    * Using Size Tiered Compaction
    * New and old data in same SSTables
    * Can't just migrate old SSTables to new cheap servers
* Solution: use Date Tiered Compaction?  
    * Requirements too strict 
    * We have "long running" records which we update over time
    * We allow customers to update historical data (tags, notes, etc)
    * Retrospection
    * Won't help with Problem #2
    * What SSTables can you safely "drop/migrate"
    * Result: FAIL

---

# Problem 2: DSE/SOLR re-index takes *forever*!
* We have nodes with up to 300GB of SOLR indexes
    * Re-index of data required to bootstrap new nodes after streaming
    * Re-index can take a week or more!!!
    * At that pace we simply can't bootstrap new nodes
* Solution:
    * Time sharded clusters in application code
    * Fanout searches for large time windows
    * Assembly results in code (using same algorithm SOLR does)
* Result: `¯\_(ツ)_/¯`
    * Interesting project
    * Works well enough
    * Big drain on developer and operations time.
    * Lots of code written, time not expended on features.

---
    
# Problem 3: Still too costly
* Cold data servers still to expensive compared to usage
* DevOps time suck is massive for rolling new shard

# Throwing ideas at the wall
* We have this giant data warehouse that doesn't cost nearly as much
    * Response time too slow: 5 seconds to pull single record by ID!
    * ETL pipeline to warehouse horribly complex and unreliable
    * ETL latency measured in hours
    * Data model way different
    * Etc, etc, etc 
* Well Facebook stores way more data than this, and it's in MySQL (ugh)
* What about Elastic Search, HBase, Hive, Parquet, MongoDB?
* Wait, hold on. This Parquet thing is interesting...

---

# Parquet

* Columnar storage format
    - High compression rate
    - Really fast for read subsets of columns and doing aggregations
* Widely used in Hadoop/Spark setups to create massive low cost data lakes
* Ecosystem support getting pretty mature

---

# Parquet: Some details

### Row Group: 
* Within each row group data is arranged by column in chunks
* One chunk for each column in the data set

### Column Chunk: 
* Chunk of data for an individual column
* Column chunks are sub-divided into pages for compression and encoding

### Page:
* Column chunks are divided up into pages for I/O

---

# Parquet Advantages:

### Columnar
* Projections are very efficient 
* Enables Vectorized execution

### Schema
* Files encode schema
* Can handle schema migrations

### Compression 
* Repeated values become cheap 
* Using Run Length Encoding
* Throw snappy, lzo, lz4, etc on top of that

---

class: center, middle

# So you have a nice file format...

# Now what?

---

class: center, middle

## Need to get data out of Cassandra:

> Spark seems good for that 

## Need to put the data somewhere:

> S3 is really cheap and fairly well supported by Spark

## Need to query the data:

> Spark seems good for that too


### Cool so just throw Spark at the problem, we are done right?

---

class: center, middle

# So we are using Spark and S3...

# Now what?

---

# Lots and lots of files 

### Sizing parquet 

--
* Parquet docs recomends 1GB files for HDFS
--

* For S3 seems to be better between 128 to 256MB

--

### We have terabytes of files
### Scans take for ever!

--

.center[![image](images/fail.png)]

---

# Partitioning

#### Our queries are always filtered by:
1. Customer 
2. Time

#### So we Partition by:

```
├── cid=X
|   ├── year=2015
|   └── year=2016
|        └── month=0
|            └── day=0
|                └── hour=0
└── cid=Y
```

### Spark understands and translates queries to this folder structure

---

class: center, middle
    
# Big Improvement:
### Now a customer can query a time range quickly

---

class: center

# Partioning problems
### Customers generally ask questions like: 

> "Over the last 6 months did how many times did I see IP X using protocol Y?"

--

> "When did IP X not use port 80 for HTTP?"

--

> "Who keeps scanning server Z for open SSH ports?"

--

### Queries would take minutes.  

---

# Partioning problems

## Queries spanning large time windows:

```sql 
select count(*) from events where ip = '192.168.0.1' and cid = 1 and year = 2016
```
--

```
├── cid=X
|   ├── year=2015
|   └── year=2016
|        |── month=0
|        |   └── ENTIRE MONTH WE COULD SKIP 
|        └── month=1
|            └── day=0
|                └── hour=0
|                    └── THE FILE WITH RESULTS!
└── cid=Y
```

---

# Problem #1     
* Requires listing out all the sub-dirs for large time ranges.
* Remember S3 is not really a file system
* **Slow!**

# Problem #2    
* Pulling potentially thousands of files from S3. 
* **Slow/Costly!**

---
 
# Solving problem #1

### Put partition info and file listings in a db ala Hive. 

### Why not just use Hive?

.center[
    ## See problem #2 
]

---

# Solving problem #2
### Still need to pull potentially thousands of files to answer our query!
--

### Can we partition differently? 
--

### By what?
--

   Field         | Cardinality   | Result
   ------------- | ------------- | --------------
   Protocol      | Medium (9000) | .center[&#x274C]
   Port          | High (65535)  | .center[&#x274C&#x274C]
   IP Addresss   | Astronomically High (3.4 undecillion) | .center[&#x274C&#x274C&#x274C]


.center[
    ### Nope! Nope! Nope!
]

---

# SOLR to the Rescue!

--

### Store file meta data in SOLR 

--

### Filter 500,000 files in < 100ms 

--

* Hive: > 5s

--

* S3 "directory" listing: **> 5 minutes!**

--

### Efficiently skip elements of partition hierarchy! 

```sql
select count(*) from events where month = 6 
```

---
class: center, middle

# Problem #1 Solved!

--

## What about Problem #2

---

# Searching High Cardinality Data

--

## Assumptions
1. Want to reduce # of files pulled for a given query
2. We are okay with a few __false posivites__

--

## This sounds like a job for... 

--

.center[
# Bloom Filters!
]

---

# Key Lookups

--

### Need to retain some C* functionality

--

### Spark/Parquet has no direct support
> What partition would it choose?

> Key would have to be encoded partition?! .big[&#x1f914;]

--

### Solution:
* Our keys have time encoded in them.  
* Enables us to generate the partition path containing the row.

--

.center[
## That was easy!
] 

---

# Other reasons to re-invent wheel
### Parquet has support for filter pushdown

--

### Spark has support for Parquet filter pushdown, but...

--

* Uses INT96 for Timestamp
    * No pushdown support: [SPARK-11784](https://issues.apache.org/jira/browse/SPARK-11784)
    * All our queries involve timestamps &#x1F622

--

* IP Addresses
    * Want to be able to push down CIDR range comparisons

--

### Lack of pushdown leads to wasted I/O and GC pressure.

---

# Offheap FTW

--

### Not using `S3AFileSystem` 
* Custom Akka HTTP based S3 impl
* Increases the parallelism I/O
* Increases parallelism of decrypting and decoding 

--

### Parsing parquet is all offheap ByteBuffers including pushdown filters
* Reduce GC pressure

--

### May have to re-vist some of this for Spark 2.0

---

# Archiving
* Currently, when Time shard fills up
    - Roll new hot time shard
    - Run spark job to Archive data to S3
    - Swap out "warm" shard for cold storage
    - Drop the "warm" shard
* Nonideal process, but deals with a lot of legacy requirements/infra
* Hoping to begin streaming data straight to cold and then only materialize 
small subsets to hot store based on customer interactions (edits, etc)

---

# What have we done so far?

--

1. Time sharded C* clusters with SOLR

--

2. Cheap speedy Cold storage based on S3 and Spark

--

3. A mechanims for archiving data to S3

---

# Thats cool, but...

--

### How do we handle queries to 3 different stores?
* C* 
* SOLR
* Spark 

--

### Handle Timesharding and Functional Sharding?

--

---

# Lot's of Scala query DSL libraries:
* Quill
* Slick
* Phantom
* etc

--

.center[ 
# AFAIK nobody supports:
]

1. Querying heterogenous data stores.

--

2. Stiching together data from heterogenous timestores.

--

3. Managing sharding:
    * Configuration 
    * Discovery

---

# Enter Quaero:

--

### Abstracts away datastore differences
* Query AST (Algebraic Data Type in Scala) 
* Command/Strategy pattern for easily plugging in new data stores

--

### Deep understanding of sharding patterns
* Handles merging of time/functional sharded data
* Adding shards is configuraton driven (no bespoke code for adding shards)

--

### Exposes a "typesafe" query DSL similar to Phantom or Rogue
* Reduce/eliminate bespoke code for retrieving the same data from all 3 stores

---

# Open Sourcing? Maybe!?
* Kind of Protectwise infra specific ATM
* Needs some cleanup
* Spark 2.0 re-work
* Spark streaming support

---
class: center, middle

# We are hiring!

> [Careers@ProtectWise](https://www.protectwise.com/careers.html)

> [@ProtectWise](https://www.protectwise.com/careers.html)

    </textarea>
    <script src="remark-latest.min.js"></script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
